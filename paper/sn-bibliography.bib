@inproceedings{Sanh2019,
abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
archivePrefix = {arXiv},
arxivId = {1910.01108},
author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
booktitle = {5th Workshop on Energy Efficient Machine Learning and Cognitive Computing @ NeurIPS 2019},
eprint = {1910.01108},
file = {:Users/shanest/Documents/Library/Sanh et al/5th Workshop on Energy Efficient Machine Learning and Cognitive Computing @ NeurIPS 2019/Sanh et al. - 2019 - DistilBERT, a distilled version of BERT smaller, faster, cheaper and lighter.pdf:pdf},
keywords = {model},
title = {{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}},
url = {http://arxiv.org/abs/1910.01108},
year = {2019}
}

@inproceedings{DevlinCLT19,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  editor = {Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter},

  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @article{cortesvapnik1995, title={Support-Vector Networks}, volume={20}, DOI={10.1007/bf00994018}, number={3}, journal={Machine Learning}, author={Cortes, Corinna and Vapnik, Vladimir}, year={1995}, pages={273–297}}
 @inproceedings{boser1992,
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
title = {A Training Algorithm for Optimal Margin Classifiers},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130385.130401},
doi = {10.1145/130385.130401},
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {144–152},
numpages = {9},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}
@InProceedings{Xu2017,
author="Xu, Shuo
and Li, Yan
and Wang, Zheng",
editor="Park, James J. (Jong Hyuk)
and Chen, Shu-Ching
and Raymond Choo, Kim-Kwang",
title="Bayesian Multinomial Na{\"i}ve Bayes Classifier to Text Classification",
booktitle="Advanced Multimedia and Ubiquitous Engineering",
year="2017",
publisher="Springer Singapore",
address="Singapore",
pages="347--352",
abstract="Text classification is the task of assigning predefined classes to free-text documents, and it can provide conceptual views of document collections. The multinomial na{\"i}ve Bayes (NB) classifier is one NB classifier variant, and it is often used as a baseline in text classification. However, multinomial NB classifier is not fully Bayesian. This study proposes a Bayesian version NB classifier. Finally, experimental results on 20 newsgroup show that Bayesian multinomial NB classifier with suitable Dirichlet hyper-parameters has similar performance with multinomial NB classifier.",
isbn="978-981-10-5041-1"
}
@article{Genkin2007,
  doi = {10.1198/004017007000000245},
  url = {https://doi.org/10.1198/004017007000000245},
  year = {2007},
  month = aug,
  publisher = {Informa {UK} Limited},
  volume = {49},
  number = {3},
  pages = {291--304},
  author = {Alexander Genkin and David D Lewis and David Madigan},
  title = {Large-Scale Bayesian Logistic Regression for Text Categorization},
  journal = {Technometrics}
}
@article{Krishnapuram2005,
  doi = {10.1109/tpami.2005.127},
  url = {https://doi.org/10.1109/tpami.2005.127},
  year = {2005},
  month = jun,
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {27},
  number = {6},
  pages = {957--968},
  author = {B. Krishnapuram and L. Carin and M.A.T. Figueiredo and A.J. Hartemink},
  title = {Sparse multinomial logistic regression: fast algorithms and generalization bounds},
  journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}
@article{Safavian1991,
  doi = {10.1109/21.97458},
  url = {https://doi.org/10.1109/21.97458},
  year = {1991},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {21},
  number = {3},
  pages = {660--674},
  author = {S.R. Safavian and D. Landgrebe},
  title = {A survey of decision tree classifier methodology},
  journal = {{IEEE} Transactions on Systems,  Man,  and Cybernetics}
}
@inproceedings{Kim2014,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}
@inproceedings{Cho2014,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}
@inproceedings{Sutskever2014,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to Sequence Learning with Neural Networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}
@inbook{Yang2019,
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {517},
numpages = {11}
}
@inproceedings{Vo2017,
  doi = {10.1109/kse.2017.8119429},
  url = {https://doi.org/10.1109/kse.2017.8119429},
  year = {2017},
  month = oct,
  publisher = {{IEEE}},
  author = {Quan-Hoang Vo and Huy-Tien Nguyen and Bac Le and Minh-Le Nguyen},
  title = {Multi-channel {LSTM}-{CNN} model for Vietnamese sentiment analysis},
  booktitle = {2017 9th International Conference on Knowledge and Systems Engineering ({KSE})}
}
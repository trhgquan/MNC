% Version 1.2 of SN LaTeX, November 2022
%
% See section 11 of the User Manual for version history 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[An Effectively Complexity-reduced Language Model for Text Classification]{DistilBERT: An Effectively Complexity-reduced Language Model for Text Classification}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,3,4]{\fnm{Ngoc-Thien-An} \sur{Pham}}\email{pntan19@clc.fitus.edu.vn}

\author[2,3,4]{\fnm{Hoang-Quan} \sur{Tran}}\email{19120338@student.hcmus.edu.vn}
\equalcont{These authors contributed equally to this work.}

% \author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

\affil*[1]{\orgdiv{Department of Computer Science}}

\affil[2]{\orgdiv{Department of Knowledge Engineering}}

\affil[3]{\orgname{Faculty of Information Technology, University of Science}}

\affil[4]{\orgname{Vietnam National University}, \orgaddress{\city{Ho Chi Minh City}, \country{Vietnam}}}


%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{Text classification has been a fundamental task dating back to the early days of Natural Language Processing and has received increased attention in recent academic literature. This task is the process of categorizing text documents into predefined classes or categories based on their content. Currently, the invention of large language models such as BERT, RoBERTa, XLM, and others has started a revolution in Natural Language Processing, specifically in Text Classification and text-related tasks. However, these models require an enormous textual dataset, a high-cost training environment, and a long training time to achieve a notable result. This paper presents a novel approach to Text Classification using a lighter, cheaper, faster version of BERT called DistilBERT. By fine-tuning DistilBERT for the Text Classification task, we achieved a state-of-the-art result of 97,40\%, 97,61\%, 97,74\%, and 97,64\% in accuracy, macro precision, macro recall, and macro F1 respectively. With these evaluation metrics, our model outperformed the current state-of-the-art language model and other approaches in the text classification task.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Text classification, Natural language processing, Deep learning, Knowledge distillation, Pretrained language models}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle


\section{Introduction}\label{introduction}
Text classification constitutes a crucial problem in the field of natural language processing (NLP). Its significance lies in its diverse range of applications, including but not limited to document organization, news filtering, spam detection, opinion mining, and computational phenotyping\cite{Aggarwal2012, Zeng2019}. Text representation plays a vital role in text classification, as it converts raw textual data into a format that can be efficiently processed by machine learning algorithms. Traditionally, text was represented using hand-crafted features such as sparse lexical features, for instance, bag-of-words and n-grams. However, recent advances in deep learning have resulted in the widespread use of convolutional neural networks (CNN)\cite{LeCun1998}, recurrent neural networks (RNN)\cite{Rumelhart1987}, and long short-term memory (LSTM)\cite{Hochreiter1997}, to learn text representations automatically. According to a recent paper by Gasparetto et al. (2022)\cite{Gasparetto2022}, using pre-trained language models such as BERT and RoBERTa can significantly improve text representation and enhance text classification performance. They reported state-of-the-art results on various benchmark datasets, demonstrating the superiority of deep learning-based methods over traditional feature-based approaches.

Although autoencoding-based pre-trained models such as BERT or RoBERTa embrace the capacity of modelling directional context at scale, the size of these models presents a noticeable challenge. Because their depth necessitates a large number of parameters that must be loaded into memory. This is not solely limited to the training phase and may pose challenges in real-world scenarios, particularly when pre-trained models are employed for inference under low-latency constraints or fine-tuning in settings with limited resources. In order to overcome this challenge, various strategies have been devised to create more compact models with fewer parameters. One successful approach to achieve this objective is through knowledge distillation. 

In this paper, based on these observations, we innovatively propose an approach to the Text Classification problem by adapting a distilled version of BERT, which is called DistilBERT. To the best of our knowledge, we are the first to adapt DistilBERT to solve the Text Classification problem. Our proposal shows its nearly equivalent performance as BERT while considerably reducing the number of parameters. Our code is available at: \href{https://github.com/trhgquan/MNC}{https://github.com/trhgquan/MNC}

The trained weights, in conjunction with the training code available in the Transformers library from HuggingFace (Wolf et al., 2019)\cite{Wolf2019}, have been employed to facilitate the fine-tuning process on domain-specific datasets.

\paragraph{Contribution}
Our contributions are summarized as follows:
We propose a knowledge distillation-based method for text classification. 
\begin{itemize}
    \item Our proposal can be more appropriate for real-world appliances by its viability of inferencing under low-latency constraints or well-functioning in limited computational resources.
    \item Our experimental results demonstrate that the proposal achieves comparable performance against those attained by state-of-the-art text classification methods on several benchmark datasets.
\end{itemize}

\paragraph{Organization}
We structure the paper into 5 sections. In Section~\ref{introduction}, the context of Text Classification is introduced and the motivation we draw to improve the performance of this problem. Next, we investigate some proposals published in recent literature in Section~\ref{relatedworks} before providing our models in detail in Section~\ref{proposedmethod}. After that, we analyze some aspects of our model in Section~\ref{experiments} prior to including some directions for future research in Section~\ref{conclusionandfutureworks}.


\section{Related works}\label{relatedworks}
In the context of Text Classification, methods are divided into two main taxonomies, including shallow and deep learning approaches. Shallow learning approaches have replaced rule-based methods in terms of accuracy and stability. While still popular in many practical contexts, these methods have limitations in processing large amounts of data. They are particularly useful when resources for deep methods are limited but require costly feature engineering, which can be challenging depending on the complexity of the domain. While deep learning models have become popular due to their capacity to extract complex features without the need for hand-engineering, thereby reducing the domain knowledge requirement. Instead, efforts have been directed towards developing neural network architectures capable of extracting effective textual representations. Recent advancements have been particularly fruitful in this regard, generating semantically meaningful and contextual representations. Automated feature extraction is particularly advantageous in modelling textual data, as it can leverage the inherent linguistic structure of a document. While the linguistic structure of a document may be intuitive to humans, it is often incomprehensible to machines.

\subsection{Shallow machine-learning models}

Early approach to the Text Classification problem using rule-based systems where an expert suggests a rule for classifying text, based on the semantics and pragmatics of the data. Later on, several methods including Naive Bayes\cite{Xu2017}, Decision Trees\cite{Safavian1991}, Logistic Regression\cite{Genkin2007} and Support Vector Machine (SVM)\cite{boser1992, cortesvapnik1995} were proposed with the concept of using the probability of tokens to predict if a document belongs to a specific class. 

These approaches are easy to implement and perform best with a small amount of data. Besides advantages, shallow models have some disadvantages by requiring a feature engineering step to be effective. Additionally, shallow learning models are depending on the probability of each token, which is not capturing the general semantics of a complex document.

\subsection{Deep learning models}
Deep learning approaches, on the other hand, learn features automatically from the data, eliminating the need for extensive feature engineering. These methods include Convolutional Neural Networks (CNN)\cite{Kim2014}, Recurrent Neural Networks (RNN)\cite{Cho2014, Sutskever2014}, combinations between CNN and RNN\cite{Vo2017}, and Transformer-based models (BERT, XLNet)\cite{Devlin2019, Yang2019}. These models have shown state-of-the-art performance on many text classification tasks but require significantly more computational resources and data to train effectively. However, these models are highly scalable, making them suitable for processing large amounts of text data.

Our research is motivated by the application of knowledge distillation-based networks\cite{Gasparetto2022} in Text Classification, as previously explored in the literature. But we further investigate several perspectives of our proposal, including its compatibility, complexity, and efficiency.

\section{Proposed method}\label{proposedmethod}
In our proposed model, the initialization of representations for tokens is achieved by using a BERT-style model (e.g, BERT, RoBERTa, or DistilBERT). DistilBERT generates token embeddings through a process known as contextualized embedding as BERT does. Specifically, the model utilizes pre-trained neural networks that have been trained on large corpora to generate context-aware word embeddings for text representations. These representations are used as inputs to DistilBERT.

Next, the outputs of DistilBERT model are a sequence of token embeddings. Subsequently, the output token \texttt{[CLS]} is fed into a final fully-connected layer with parameters $W \in \mathbb{R}^{D\times H}$ and $b \in \mathbb{R}^H$. In that, $D$ is the dimensionality of the output vector and H denotes the number of classes needed for classifying. Finally, to predict the label, the softmax layer is applied to the new projected vector. During fine-tuning, the cross-entropy and binary cross-entropy loss functions are employed for the minimization process in single-label and multi-label tasks, respectively. The overall framework of our model is depicted in the Figure~\ref{fig:distilbert}. Depending on each type of dataset, these loss functions are based on the comparison between the predicted probability distribution and the actual label distribution, with the objective of minimizing the discrepancy between them.

\begin{figure}
\centering
\includegraphics[scale=.4]{distilbert.png}
\caption{Our proposed method using DistilBERT architecture}
\label{fig:distilbert}
\end{figure}

\section{Experiments}\label{experiments}
\subsection{Baselines}
We compare our proposed model against multiple state-of-the-art text classification methods:
\begin{itemize}
\item \textbf{CNN}: The Convolutional Neural Network (CNN) approach was introduced by Kim et al (2014)\cite{Kim2014}. Using an unsupervised learning model, an input word is converted into a word representation vector in a contextual space. These vectors are then stacked together to form a sentence-representation matrix. Finally, using a simple CNN with one layer of convolution on top of pre-trained word vectors.
\item \textbf{RNN}: Cho et al. (2014) and Sutskever et al. (2014)\cite{Cho2014, Sutskever2014} proposed an architecture named Sequence-to-Sequence (Seq2Seq) to solve the Machine Translation problem, then used in Text Classification.
\item \textbf{CNN + LSTM}: A combination of two previous well-known approaches was introduced by Vo et al. (2017)\cite{Vo2017} for Vietnamese Text Classification. An input sentence is fed into two different architectures including a CNN with different filter sizes and an LSTM Sequence-to-Sequence Encoder. The output vector of CNN is then concatenated with the hidden state output of the LSTM network to form a combination context vector, which would go through a fully connected layer to get the final probability distribution over classes.
\item \textbf{Transformer}: Vaswani et al. (2017)\cite{Vaswani2017} introduced Transformer as an effective architecture for Machine Translation and can be applied to various downstream tasks. Some advantages of Transformer compared to Sequence-to-Sequence models include capturing essential information, eliminating recurrent connection, and supporting parallel computing.
\item \textbf{BERT}: The Encoder of Transformer is then used to create an independent language model called Bidirectional Encoder Representations from Transformers (BERT)\cite{Devlin2019}.
\item \textbf{XLNet}: Although XLNet can be classified as an autoregressive model, it overcomes the lack of bi-directionality of previous autoregressive models. This model utilizes a generalized autoregressive pre-training methodology to determine the likelihood of a word token\cite{Yang2019}.
\end{itemize}

\subsection{Dataset}
\subsubsection{The TREC dataset}
Experiments were conducted on the TREC dataset\cite{hovy2001, li2002} including 6000 sentences divided into training/testing sets with 5500 for training and 500 for testing. We use the TREC training set for training and finetuning while evaluations were made on the testing set. The dataset’s statistics are briefly described in Table~\ref{tab:trec-insight}

\begin{table}
\centering
\caption{Insight of the TREC dataset} \label{tab:trec-insight}
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}cccccc}
\toprule
\textbf{Training} & \textbf{Testing} & \textbf{Coarse labels} & \textbf{Fine class labels} & \textbf{Avg. length} & \textbf{Vocab. size} \\ \midrule
5500              & 500              & 6                      & 50                         & 10                   & 8700                 \\ \bottomrule
\end{tabular*}
\end{table}

\subsubsection{The IMDb dataset}

\subsection{Experiment setups}
To guarantee objectivity and fairness, we run our experiments five times. The results nearly remain the same. We initialize the learning rate to 2e-3 and the weight decay to 0.01 for DistilBERT. 

\subsubsection{Metrics}
We use Accuracy, Macro Precision, Macro Recall and Macro F1 Score as evaluation metrics. The definition of each metric is presented in the following sections.

\paragraph{Accuracy}
Assuming the testing set has $n$ samples, let $\hat{y}_i$ and $y_i$ be the $i$-th sentence predicted label and ground-truth label respectively. Hence, the Accuracy metric will be defined as total correctly predicted labels divided by total labels, defined in Equation~\ref{eq:accuracy}.

\begin{equation}\label{eq:accuracy}
\text{Accuracy} = \frac{1}{n}\displaystyle\sum_{i = 1}^n (y_i = \hat{y_i})
\end{equation}

\paragraph{Precision, Recall and F1 Score}
Given a class $C$, let $TP$ be the total correctly-labelled samples of class $C$ (\textit{true positive} samples); $FP$ be the total samples not belonging to but incorrectly labelled as $C$ (\textit{false positive} samples), and $FN$ be the total samples belonging to $C$ but incorrectly labelled as another class (\textit{false negative} samples). Hence, the Precision metric would be defined as Equation~\ref{eq:precision}
\begin{equation}\label{eq:precision}
\text{Precision (P)} = \frac{TP}{TP + FP}    
\end{equation}
the Recall metric would be defined as Equation~\ref{eq:recall}
\begin{equation}\label{eq:recall}
\text{Recall (R)} = \frac{TP}{TP + FN}
\end{equation}
and the F1 Score would be defined as Equation~\ref{eq:f1}
\begin{equation}\label{eq:f1}
\text{F1} = \frac{2PR}{P + R}
\end{equation}

\paragraph{Micro and Macro metrics}
Take an example of a classification problem with $n$ labels using Precision metric. With $TP_1, TP_2, .., TP_n$; $FP_1, FP_2, .., FP_n$ are total \textit{true positive} and \textit{false positive} samples of $n$ classes. The Micro Precision and Macro Precision metrics would be defined as Equation~\ref{eq:micro-macro-precision}

\begin{equation}\label{eq:micro-macro-precision}
\begin{aligned}
\text{Precision}_{micro} &= \frac{TP_1 + TP_2 + .. + TP_n}{(TP_1 + FP_1) + (TP_2 + FP_2) + .. + (TP_n + FP_n)} \\
&= \frac{\displaystyle\sum_{i = 1}^n TP_i}{\displaystyle\sum_{i = 1}^n (TP_i + FP_i)} \\
\text{Precision}_{macro} &= \frac{1}{n}\left[\left(\frac{TP_1}{TP_1 + FP_1}\right) + \left(\frac{TP_2}{TP_2 + FP_2}\right) + .. + \left(\frac{TP_n}{TP_n + FP_n}\right)\right] \\
&= \frac{1}{n}\sum_{i = 1}^n \frac{TP_i}{TP_i + FP_i}
\end{aligned}
\end{equation}

\subsubsection{Results}
We use the same initial random seeds for every experiment. Table~\ref{tab:result} reports the Accuracy, Macro Precision, Macro Recall and Macro F1 of the baselines and the proposed method on both the TREC and IMDb testing sets.

\begin{sidewaystable}
\caption{Experiment results on the TREC dataset} \label{tab:result}
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}lcccc}
\toprule
\multicolumn{1}{c}{\textbf{Model}}                    & \multicolumn{1}{c}{\textbf{Accuracy}} & \multicolumn{1}{c}{\textbf{Precision (macro)}} & \multicolumn{1}{c}{\textbf{Recall (macro)}} & \multicolumn{1}{c}{\textbf{F1 score (macro)}} \\ \midrule
Logistic Regression                                   & 0.852                                 & 0.831                                          & 0.897                                       & 0.856                                         \\
Multinomial Naive Bayes                               & 0.832                                 & 0.704                                          & 0.699                                       & 0.697                                         \\
Support Vector Classifier                             & 0.886                                 & 0.862                                          & 0.912                                       & 0.882                                         \\ \midrule
BiGRU Seq2Seq            & 0.836                                 & 0.695                                          & 0.708                                       & 0.700                                         \\ \midrule
CNN + random embedding                 & 0.726                                 & 0.809                                          & 0.684                                       & 0.718                                         \\
CNN + fastText (freezed)                           & 0.924                                 & 0.933                                          & 0.898                                       & 0.912                                         \\
CNN + fastText (trainable)                         & 0.910                                 & 0.922                                          & 0.885                                       & 0.899                                         \\
\midrule
CNN + LSTM                           & 0.652                                 & 0.717                                          & 0.637                                       & 0.631                                         \\
CNN + BiGRU + random embedding     & 0.676                                 & 0.717                                          & 0.701                                       & 0.689                                         \\
CNN + BiGRU + fastText (freezed)   & 0.914                                 & 0.919                                          & 0.896                                       & 0.904                                         \\
CNN + BiGRU + fastText (trainable) & 0.902                                 & 0.892                                          & 0.884                                       & 0.886                                         \\ \midrule
Finetuned BERT                                 & 0.966                                 & 0.971                                          & 0.970                                       & 0.970                                         \\
Finetuned XLM-RoBERTa                                 & 0.966                                 & 0.971                                          & 0.970                                       & 0.970                                         \\ \midrule
Finetuned DistilBERT                                  & \textbf{0.974}                        & \textbf{0.976}                                 & \textbf{0.977}                              & \textbf{0.976}                                \\ \bottomrule
\end{tabular*}
\end{sidewaystable}

We measured the inference time on the TREC testing set using the Google Colab environment. DistilBERT performs slower than CNN and Seq2Seq approaches with the same initial seed but is faster than approaches using LLMs. The inference time of each model is presented in Table~\ref{tab:inference-TREC}

\begin{table}
\caption{Inference time on the TREC testing set}\label{tab:inference-TREC}
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}lc}
\toprule
\textbf{Model}                     & \textbf{Inference time (mean $\pm$ stdev)} \\ \midrule
BiGRU Seq2Seq                      & 1.8119 $\pm$ 0.6681 ms                     \\ \midrule
CNN + random embedding             & 1.6734 $\pm$ 0.3585 ms                     \\
CNN + fastText (freezed)           & 1.6098 $\pm$ 0.3179 ms                     \\
CNN + fastText (trainable)         & 1.6577 $\pm$ 0.4441 ms                     \\ \midrule
CNN + LSTM + random embedding      & 5.6255 $\pm$ 0.7504 ms                     \\
CNN + BiGRU + random embedding     & 11.3872 $\pm$ 3.0772 ms                    \\
CNN + BiGRU + fastText (freezed)   & 10.2113 $\pm$ 1.4602 ms                    \\
CNN + BiGRU + fastText (trainable) & 10.2700 $\pm$ 2.1868 ms                    \\ \midrule
Finetuned BERT                     & 17.7249 $\pm$ 4.0707 ms                    \\
Finetuned XLM-RoBERTa              & 16.9330 $\pm$ 8.9874 ms                    \\ \midrule
Finetuned DistilBERT               & 9.7757 $\pm$ 2.4985 ms                     \\ \bottomrule
\end{tabular*}
\end{table}

\subsection{Analysis}
sede vacante

\section{Conclusion and Future works}\label{conclusionandfutureworks}
In this work, we propose DistilBERT, a cheaper, lighter, and faster version of BERT, for Text Classification. Even though reducing the number of layers (e.g., attention heads), DistilBERT still shows its efficiency by the comparable achievements. Besides, we analyze deeper into related aspects of this knowledge distillation-based model, including performance, inference, and complexity. However, in this work, we only focus on investigating the architecture used without considering how to integrate more useful information (e.g., structural or hierarchical) which is crucial for the model to capture more meaningful contextual features. We leave this for future research.

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}

\section{Introduction}
% \subsection{Overview}
Text classification is a fundamental problem in the field of natural language processing (NLP), which involves assigning predefined categories to a given text based on its content. It has a wide range of applications such as document organisation, news filtering, spam detection, opinion mining, and computational phenotyping.  In that, text representation is a crucial intermediate step. Traditionally, text is represented using manually engineered features such as sparse lexical features, for example, bag-of-words and n-grams. However, in recent times, deep learning models have become popular for automatically learning text representations. 

% \subsection{Aims and Objectives}
In the context of Text Classification, methods are divided into two main taxonomies, including shallow and deep learning approaches. Shallow learning approaches have replaced rule-based methods in terms of accuracy and stability. While still popular in many practical contexts, these methods have limitations in processing large amounts of data. They are particularly useful when resources for deep methods are limited, but require costly feature engineering, which can be challenging depending on the complexity of the domain. While deep learning models have become popular due to their capacity to extract complex features without the need for hand-engineering, thereby reducing the domain knowledge requirement. Instead, efforts have been directed towards developing neural network architectures capable of extracting effective textual representations. Recent advancements have been particularly fruitful in this regard, generating semantically meaningful and contextual representations. Automated feature extraction is particularly advantageous in modelling textual data, as it can leverage the inherent linguistic structure of a document. While the linguistic structure of a document may be intuitive to humans, it is often incomprehensible to machines.

The size of deep language models presents a noticeable challenge, as their depth necessitates a large number of parameters that must be loaded into memory. This is not solely limited to the training phase and may pose challenges in real-world scenarios, particularly when pretrained models are employed for inference under low-latency constraints or fine-tuning in settings with limited resources.In order to overcome this challenge, various strategies have been devised to create more compact models with fewer parameters. One successful approach to achieve this objective is through knowledge distillation. Based on these observations, we propose an innovative approach to the Text Classification problem by adapting a distilled version of BERT\cite{DevlinCLT19}, a well-established NLP model, which is called DistilBERT\cite{Sanh2019}.


\paragraph{Keywords}
\textit{Natural language processing, Deep learning, Knowledge distillation, Pretrained language models}